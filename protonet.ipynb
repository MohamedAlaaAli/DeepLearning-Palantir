{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import Omniglot\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotFewShot(Dataset):\n",
    "    def __init__(self, root, mode=\"train\", transform=None, n_way=5, k_shots=5, n_query=5):\n",
    "        super(OmniglotFewShot, self).__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.n_way = n_way\n",
    "        self.k_shots = k_shots\n",
    "        self.n_query = n_query\n",
    "        self.mode = mode\n",
    "        self.data = Omniglot(root=self.root, background=True if self.mode == \"train\" else False, download=True, transform=self.transform)\n",
    "        self.indices_by_class = self._create_indices_by_class()\n",
    "\n",
    "    def _create_indices_by_class(self):\n",
    "        indices_by_class = {}\n",
    "        for idx, (_, label) in enumerate(self.data):\n",
    "            if label not in indices_by_class:\n",
    "                indices_by_class[label] = []\n",
    "            indices_by_class[label].append(idx)\n",
    "        return indices_by_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_by_class)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        class_indices = np.random.choice(list(self.indices_by_class.keys()), self.n_way, replace=False)\n",
    "        \n",
    "        support_set = []\n",
    "        query_set = []\n",
    "        support_labels = []\n",
    "        query_labels = []\n",
    "\n",
    "        for class_index in class_indices:\n",
    "            indices = np.random.choice(self.indices_by_class[class_index], self.k_shots + self.n_query, replace=False)\n",
    "            class_support_set_indices = indices[:self.k_shots]\n",
    "            class_query_set_indices = indices[self.k_shots:]\n",
    "\n",
    "            for i in class_support_set_indices:\n",
    "                image, _ = self.data[i]\n",
    "                support_set.append(image)\n",
    "                support_labels.append(class_index)\n",
    "\n",
    "            for i in class_query_set_indices:\n",
    "                image, _ = self.data[i]\n",
    "                query_set.append(image)\n",
    "                query_labels.append(class_index)\n",
    "\n",
    "        # Convert lists to tensors for PyTorch compatibility\n",
    "        support_set = torch.stack(support_set)\n",
    "        query_set = torch.stack(query_set)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "\n",
    "\n",
    "        support_set = support_set.to(device)\n",
    "        query_set = query_set.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "\n",
    "        return support_set, query_set, support_labels, query_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation90:\n",
    "    def __call__(self, img):\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        return transforms.functional.rotate(img, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    RandomRotation90(),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = OmniglotFewShot(root='./data', mode='train', transform=transform, n_way=60, k_shots=5, n_query=5)\n",
    "test_dataset = OmniglotFewShot(root='./data', mode='test', transform=transform, n_way=5, k_shots=5, n_query=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.in_channels = 1\n",
    "        self.out_channels = 64\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Protonet(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super(Protonet, self).__init__()\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def forward(self, support, query, n_way, k_shots):\n",
    "        \"\"\"\n",
    "        Perform the forward pass and compute the prototypes.\n",
    "\n",
    "        Parameters:\n",
    "        - support: The support set.\n",
    "        - query: The query set.\n",
    "        - n_way: The number of classes (ways).\n",
    "        - k_shots: The number of examples per class in the support set.\n",
    "\n",
    "        Returns:\n",
    "        - query_embeddings: The embeddings of the query set.\n",
    "        - prototypes: The class prototypes.\n",
    "        \"\"\"\n",
    "        # Embed support and query sets\n",
    "        print(support.shape)\n",
    "        support_embeddings = self.embedder(support.squeeze(0))\n",
    "        query_embeddings = self.embedder(query.squeeze(0))\n",
    "        \n",
    "        # Calculate the prototypes for each class in the support set\n",
    "        # Reshape support embeddings to [n_way, k_shots, embedding_size] and compute mean across k_shots\n",
    "        prototypes = support_embeddings.view(n_way, k_shots, -1).mean(dim=1)\n",
    "        \n",
    "        return query_embeddings, prototypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Protonet(Embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protonet_loss(query_embeddings, prototypes, query_labels, n_way):\n",
    "    \"\"\"\n",
    "    Computes the Prototypical Networks loss given embeddings of the query set,\n",
    "    class prototypes, and query labels.\n",
    "\n",
    "    Parameters:\n",
    "    - query_embeddings: The embeddings of the query set.\n",
    "    - prototypes: The class prototypes.\n",
    "    - query_labels: The labels for the query set.\n",
    "    - n_way: The number of classes (ways).\n",
    "\n",
    "    Returns:\n",
    "    - Loss calculated using Negative Log Likelihood.\n",
    "    \"\"\"\n",
    "    # Calculate the Euclidean distance from each query sample to the prototypes\n",
    "    distances = torch.cdist(query_embeddings, prototypes) # shape n_instances,n_ways\n",
    "    \n",
    "    # Find the index of the nearest centroid for each instance\n",
    "    nearest_indices = torch.argmin(distances, dim=1) # min_dist_index\n",
    "    print(query_labels[nearest_indices])\n",
    "    # Convert query_labels to a 1D tensor\n",
    "    query_labels = query_labels.view(-1)\n",
    "    \n",
    "    # Convert distances to log probabilities\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)  # Apply softmax and take log\n",
    "    \n",
    "    # Compute the negative log likelihood loss\n",
    "    loss = F.nll_loss(log_p_y, query_labels)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=2000, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/964 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300, 1, 28, 28])\n",
      "------------\n",
      "tensor([[545, 545, 545, 545, 545, 479, 479, 479, 479, 479, 200, 200, 200, 200,\n",
      "         200, 703, 703, 703, 703, 703, 904, 904, 904, 904, 904, 374, 374, 374,\n",
      "         374, 374, 656, 656, 656, 656, 656,  49,  49,  49,  49,  49, 760, 760,\n",
      "         760, 760, 760, 599, 599, 599, 599, 599, 935, 935, 935, 935, 935, 658,\n",
      "         658, 658, 658, 658,  87,  87,  87,  87,  87, 349, 349, 349, 349, 349,\n",
      "         879, 879, 879, 879, 879,  12,  12,  12,  12,  12, 478, 478, 478, 478,\n",
      "         478, 246, 246, 246, 246, 246, 542, 542, 542, 542, 542, 721, 721, 721,\n",
      "         721, 721,  14,  14,  14,  14,  14, 445, 445, 445, 445, 445, 594, 594,\n",
      "         594, 594, 594, 591, 591, 591, 591, 591, 357, 357, 357, 357, 357, 733,\n",
      "         733, 733, 733, 733, 644, 644, 644, 644, 644, 135, 135, 135, 135, 135,\n",
      "         170, 170, 170, 170, 170, 175, 175, 175, 175, 175, 632, 632, 632, 632,\n",
      "         632, 746, 746, 746, 746, 746, 100, 100, 100, 100, 100, 194, 194, 194,\n",
      "         194, 194, 882, 882, 882, 882, 882,  35,  35,  35,  35,  35, 497, 497,\n",
      "         497, 497, 497, 252, 252, 252, 252, 252, 603, 603, 603, 603, 603, 426,\n",
      "         426, 426, 426, 426,  53,  53,  53,  53,  53, 181, 181, 181, 181, 181,\n",
      "         775, 775, 775, 775, 775, 173, 173, 173, 173, 173, 433, 433, 433, 433,\n",
      "         433, 223, 223, 223, 223, 223, 421, 421, 421, 421, 421, 712, 712, 712,\n",
      "         712, 712, 659, 659, 659, 659, 659,   9,   9,   9,   9,   9, 769, 769,\n",
      "         769, 769, 769,  16,  16,  16,  16,  16, 687, 687, 687, 687, 687, 177,\n",
      "         177, 177, 177, 177, 661, 661, 661, 661, 661, 911, 911, 911, 911, 911,\n",
      "         678, 678, 678, 678, 678, 844, 844, 844, 844, 844, 905, 905, 905, 905,\n",
      "         905,  15,  15,  15,  15,  15]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/964 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_labels)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mprotonet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mprotonet_loss\u001b[0;34m(query_embeddings, prototypes, query_labels, n_way)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Find the index of the nearest centroid for each instance\u001b[39;00m\n\u001b[1;32m     19\u001b[0m nearest_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(distances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# min_dist_index\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnearest_indices\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert query_labels to a 1D tensor\u001b[39;00m\n\u001b[1;32m     22\u001b[0m query_labels \u001b[38;5;241m=\u001b[39m query_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "total_episodes = 0  # Initialize total episodes\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch')\n",
    "    \n",
    "    for support_set, query_set, support_labels, query_labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        query_embeddings, prototypes = model(support_set, query_set, 60, 5)\n",
    "\n",
    "        print(\"------------\")\n",
    "        print(query_labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = protonet_loss(query_embeddings, prototypes, query_labels, 60)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar description with current loss\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        \n",
    "        total_episodes += 1  # Increment total episodes\n",
    "        scheduler.step(total_episodes)  # Update learning rate\n",
    "        \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Avg. Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Training complete\n",
    "print('Training complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
